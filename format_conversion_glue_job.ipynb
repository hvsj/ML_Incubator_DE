{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca3de32-e7bf-4193-806d-3719dff493d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import boto3\n",
    "    from pathlib import Path, PurePosixPath\n",
    "    from typing import Union, TypeVar\n",
    "    \n",
    "    from pyspark.sql.session import SparkSession\n",
    "    \n",
    "    from awsglue.transforms import *\n",
    "    from awsglue.utils import getResolvedOptions\n",
    "    from pyspark.context import SparkContext\n",
    "    from awsglue.context import GlueContext\n",
    "    from awsglue.job import Job\n",
    "except exception as e:\n",
    "    print(\"Unable to import modules: \", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d0ca35d-2bdc-4155-b29f-da2cfdd7c06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_csv_to_parquet(source_path: str, target_path: str):\n",
    "    \"\"\"\n",
    "    This function will take a csv file as input and convert it to parquet file.\n",
    "    :param source_path: source file path\n",
    "    :param target_path: target file path\n",
    "    :return: It returns either True or False regarding the status of conversion and \n",
    "                its respective error message if it fails \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        print(\"Source Path: \" + source_path)\n",
    "        csv_datasource = glueContext.create_dynamic_frame.from_options(\n",
    "            connection_type=\"s3\",\n",
    "            connection_options={\"paths\": [source_path]},\n",
    "            format=\"csv\",\n",
    "            format_options={\"withHeader\": True}\n",
    "        )\n",
    "\n",
    "        # Convert the CSV dynamic frame to a Spark DataFrame\n",
    "        # csv_dataframe = csv_datasource.toDF()\n",
    "\n",
    "        # Write the DataFrame in Parquet format\n",
    "        write_parquet_out = glueContext.write_dynamic_frame.from_options(\n",
    "            # frame=csv_dataframe,\n",
    "            frame=csv_datasource,\n",
    "            connection_type=\"s3\",\n",
    "            connection_options={\"path\": target_path, \"partitionKeys\": []},\n",
    "            format=\"parquet\",\n",
    "            format_options={\n",
    "                #\"compression\": \"gzip\"\n",
    "                \"useGlueParquetWriter\": True\n",
    "            },\n",
    "            transformation_ctx=\"write_parquet_out\",\n",
    "            )\n",
    "\n",
    "        # We can also use DataFrames in a script (pyspark.sql.DataFrame).\n",
    "        # Write the DataFrame in Parquet format\n",
    "        # csv_dataframe.write.parquet(TARGET_PATH, mode=\"overwrite\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\")\n",
    "        print(e)\n",
    "        return False, \"Unable to convert CSV file to PARQUET, please find detailed exception in exception message!!\", str(e)\n",
    "    else:\n",
    "        print(\"Source Path: \" + target_path)\n",
    "        return True, \"Succesfully converted CSV file to PARQUET\", None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2687841a-a8ae-4a94-9f8b-eeb6b1f0641a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_list_of_csv_files(s3_client, bucket_name, sorce_folder_name, target_folder_name):\n",
    "    try:    \n",
    "        dict_of_csv_files: list = {}\n",
    "\n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=bucket_name,\n",
    "            Prefix=sorce_folder_name\n",
    "        )\n",
    "\n",
    "        # Print the file names\n",
    "        if \"Contents\" in response:\n",
    "            for file in response[\"Contents\"]:\n",
    "                # ignoring folders\n",
    "                if not file['Key'].endswith('/'):\n",
    "                    print(\"*\"*20)\n",
    "\n",
    "                    file_name = Path(file['Key']).name\n",
    "                    print(\"filename: \" + file_name)\n",
    "\n",
    "                    source_file_path = S3_PATH_STRCTURE.format(bucket_name=bucket_name, file_name=file['Key'])\n",
    "                    print(\"Source filepath: \" + source_file_path)\n",
    "\n",
    "                    target_subfolder_name = str(PurePosixPath(file['Key']).parent.name)\n",
    "                    target_path_name = f\"{target_folder_name}{target_subfolder_name}/\"\n",
    "                    target_file_path = S3_PATH_STRCTURE.format(bucket_name=bucket_name, file_name=target_path_name)\n",
    "                    print(\"Target filepath: \" + target_file_path)\n",
    "\n",
    "                    dict_of_csv_files[file_name] = {\n",
    "                        \"FILE_NAME\": file_name,\n",
    "                        \"SOURCE_PATH\": source_file_path,\n",
    "                        \"TARGET_PATH\": target_file_path\n",
    "                    }\n",
    "\n",
    "                    print(\"*\"*20)\n",
    "\n",
    "            return True, \"Found some files in folder\", dict_of_csv_files\n",
    "\n",
    "        else:\n",
    "            print(\"No files found in the folder.\")\n",
    "            return False, \"No files found in the folder.\", []\n",
    "        \n",
    "    except exception as e:\n",
    "        print(\"Exception:\")\n",
    "        print(e)\n",
    "        raise Exception(e)\n",
    "        return False, str(e), []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4df41465-4980-4085-966f-eb177950ee4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/glue_user/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn("
     ]
    }
   ],
   "source": [
    "# Create a Spark context\n",
    "# sc = SparkContext()\n",
    "# sc = spark.sparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Get the job arguments \n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "\n",
    "# Create a Glue job\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478dda8a-f47a-4cea-9258-4229a00fadd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AWS_PROFILE_NAME = \"ml-inc-gp-profile\"\n",
    "S3_PATH_STRCTURE: str = \"s3://{bucket_name}/{file_name}\"\n",
    "\n",
    "BUCKET_NAME = \"feedin-bkt\"\n",
    "SOURCE_FOLDER_NAME = \"covid-19-testing-data/dataset/csv/\"\n",
    "TARGET_FOLDER_NAME = \"covid-19-testing-data/dataset/parquet/\"\n",
    "\n",
    "# Create a session with the specified profile\n",
    "session = boto3.Session(profile_name=AWS_PROFILE_NAME)\n",
    "\n",
    "# Create an S3 client using the session\n",
    "s3_client = session.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98079de6-acb0-484f-9a45-3ea515358f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "filename: states_current.csv\n",
      "Source filepath: s3://feedin-bkt/covid-19-testing-data/dataset/csv/states_current/states_current.csv\n",
      "Target filepath: s3://feedin-bkt/covid-19-testing-data/dataset/parquet/states_current/\n",
      "********************\n",
      "********************\n",
      "filename: states_daily.csv\n",
      "Source filepath: s3://feedin-bkt/covid-19-testing-data/dataset/csv/states_daily/states_daily.csv\n",
      "Target filepath: s3://feedin-bkt/covid-19-testing-data/dataset/parquet/states_daily/\n",
      "********************\n",
      "********************\n",
      "filename: states_info.csv\n",
      "Source filepath: s3://feedin-bkt/covid-19-testing-data/dataset/csv/states_info/states_info.csv\n",
      "Target filepath: s3://feedin-bkt/covid-19-testing-data/dataset/parquet/states_info/\n",
      "********************\n",
      "********************\n",
      "filename: states_screenshots.csv\n",
      "Source filepath: s3://feedin-bkt/covid-19-testing-data/dataset/csv/states_screenshots/states_screenshots.csv\n",
      "Target filepath: s3://feedin-bkt/covid-19-testing-data/dataset/parquet/states_screenshots/\n",
      "********************\n",
      "********************\n",
      "filename: us_current.csv\n",
      "Source filepath: s3://feedin-bkt/covid-19-testing-data/dataset/csv/us_current/us_current.csv\n",
      "Target filepath: s3://feedin-bkt/covid-19-testing-data/dataset/parquet/us_current/\n",
      "********************\n",
      "********************\n",
      "filename: us_daily.csv\n",
      "Source filepath: s3://feedin-bkt/covid-19-testing-data/dataset/csv/us_daily/us_daily.csv\n",
      "Target filepath: s3://feedin-bkt/covid-19-testing-data/dataset/parquet/us_daily/\n",
      "********************\n",
      "{'states_current.csv': {'FILE_NAME': 'states_current.csv', 'SOURCE_PATH': 's3://feedin-bkt/covid-19-testing-data/dataset/csv/states_current/states_current.csv', 'TARGET_PATH': 's3://feedin-bkt/covid-19-testing-data/dataset/parquet/states_current/'}, 'states_daily.csv': {'FILE_NAME': 'states_daily.csv', 'SOURCE_PATH': 's3://feedin-bkt/covid-19-testing-data/dataset/csv/states_daily/states_daily.csv', 'TARGET_PATH': 's3://feedin-bkt/covid-19-testing-data/dataset/parquet/states_daily/'}, 'states_info.csv': {'FILE_NAME': 'states_info.csv', 'SOURCE_PATH': 's3://feedin-bkt/covid-19-testing-data/dataset/csv/states_info/states_info.csv', 'TARGET_PATH': 's3://feedin-bkt/covid-19-testing-data/dataset/parquet/states_info/'}, 'states_screenshots.csv': {'FILE_NAME': 'states_screenshots.csv', 'SOURCE_PATH': 's3://feedin-bkt/covid-19-testing-data/dataset/csv/states_screenshots/states_screenshots.csv', 'TARGET_PATH': 's3://feedin-bkt/covid-19-testing-data/dataset/parquet/states_screenshots/'}, 'us_current.csv': {'FILE_NAME': 'us_current.csv', 'SOURCE_PATH': 's3://feedin-bkt/covid-19-testing-data/dataset/csv/us_current/us_current.csv', 'TARGET_PATH': 's3://feedin-bkt/covid-19-testing-data/dataset/parquet/us_current/'}, 'us_daily.csv': {'FILE_NAME': 'us_daily.csv', 'SOURCE_PATH': 's3://feedin-bkt/covid-19-testing-data/dataset/csv/us_daily/us_daily.csv', 'TARGET_PATH': 's3://feedin-bkt/covid-19-testing-data/dataset/parquet/us_daily/'}}\n",
      "Succesfully converted CSV filr to PARQUET\n",
      "Succesfully converted CSV filr to PARQUET\n",
      "Succesfully converted CSV filr to PARQUET\n",
      "Succesfully converted CSV filr to PARQUET\n",
      "Succesfully converted CSV filr to PARQUET\n",
      "Succesfully converted CSV filr to PARQUET"
     ]
    }
   ],
   "source": [
    "# need to iterate through all the csv files in csv folder and the convert them to parquet files\n",
    "\n",
    "fun_response, fun_status_msg, dict_of_csv_files = get_list_of_csv_files(s3_client=s3_client, bucket_name=BUCKET_NAME, sorce_folder_name=SOURCE_FOLDER_NAME, target_folder_name=TARGET_FOLDER_NAME)\n",
    "\n",
    "if fun_status_msg is False:\n",
    "    print(fun_status_msg)\n",
    "    print()\n",
    "    print(\"No files to convert into parqet files!!\")\n",
    "else:\n",
    "    print(dict_of_csv_files)\n",
    "    \n",
    "    for key, value in dict_of_csv_files.items():\n",
    "        \n",
    "        source_path = value[\"SOURCE_PATH\"]\n",
    "        target_path = value[\"TARGET_PATH\"]\n",
    "        fun_response, fun_status_msg, fun_exception_msg = convert_csv_to_parquet(source_path=source_path, target_path=target_path)\n",
    "\n",
    "        if fun_response is False:\n",
    "            raise Exception(fun_exception_msg)\n",
    "\n",
    "        print(fun_status_msg)\n",
    "    \n",
    "# Commit the job\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
